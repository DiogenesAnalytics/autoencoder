{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b22915-aa21-45cd-a740-0b7a0a058ae9",
   "metadata": {},
   "source": [
    "# tf_flowers Dataset\n",
    "The following `Jupyter Notebook` has been *adapted* from the [Keras blog article](https://blog.keras.io/building-autoencoders-in-keras.html) written by *F. Chollet* on [autoencoders](https://en.wikipedia.org/wiki/Autoencoder), and will focus on training the `Minimal2DAutoencoder` on the `tf_flowers` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c142c-a636-4e24-ab0e-41332c6025a2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Need to get the necessary packages ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e44f45-3f89-41ef-8e47-e4f682906e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for colab\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "  # install colab dependencies\n",
    "  !pip install git+https://github.com/DiogenesAnalytics/autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa8c90-fb65-40c0-b198-22cef85d94d6",
   "metadata": {},
   "source": [
    "## Get tf_flowers Data\n",
    "The [TensorFlow Flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers) dataset first needs to be downloaded, and then preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c6473-6b1c-4e49-add3-cc463e7f14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial libs\n",
    "import keras\n",
    "import pathlib\n",
    "\n",
    "# data location\n",
    "DATASET_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "# download, get path, and convert to pathlib obj\n",
    "data_dir = pathlib.Path(\n",
    "    keras.utils.get_file(\"flower_photos\", origin=DATASET_URL, untar=True, cache_dir=\"./data/keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d4de9-fc78-4e1f-b456-2cb2cf94a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use keras util to load raw images into tensorflow.data.Dataset\n",
    "x_train, x_val = keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  color_mode=\"rgb\",\n",
    "  validation_split=(1 / 7),\n",
    "  subset=\"both\",\n",
    "  seed=42,\n",
    "  image_size=(28, 28),\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# use a keras layer to normalize\n",
    "normalization_layer = keras.layers.Rescaling(1./255)\n",
    "\n",
    "# will apply normalization as well as regrouping each element\n",
    "def change_inputs(images, labels):\n",
    "  x = normalization_layer(images)\n",
    "  return x, x\n",
    "\n",
    "# apply change\n",
    "x_train, x_val = x_train.map(change_inputs), x_val.map(change_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd90e5-b41d-4c88-b57e-4326ffa71352",
   "metadata": {},
   "source": [
    "## Autoencoder Training\n",
    "Finally the *autoencoder* can be trained ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57935af9-4b4b-4e9e-9e61-502b3c391063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get libs for training ae\n",
    "from autoencoder.model.minimal import Min2DAE, Min2DParams\n",
    "\n",
    "# seupt config\n",
    "config = Min2DParams(\n",
    "    l0={\"input_shape\": (28, 28, 3)},\n",
    "    l2={\"units\": 32 * 3},\n",
    "    l3={\"units\": 28 * 28 * 3},\n",
    "    l4={\"target_shape\": (28, 28, 3)},\n",
    ")\n",
    "\n",
    "# get ae instance\n",
    "autoencoder = Min2DAE(config)\n",
    "\n",
    "# check network topology\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b05f10-e00b-466a-a5e7-6cda75299108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get libs for early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# create callback early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "\n",
    "# compile ...\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# ... and train\n",
    "history = autoencoder.fit(\n",
    "    x=x_train,\n",
    "    validation_data=x_val,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6a3a8-49ae-439e-8abe-c7b4531601e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show training loss\n",
    "autoencoder.training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dffb64-abda-467d-8fe9-bb5f28d12a3f",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "Now it is possible, using the trained autoencoder, to encode/decode an image and see how it compares to the original ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a4180-ca54-437d-abc4-21427f45eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get viz func\n",
    "from autoencoder.data import compare_image_predictions\n",
    "\n",
    "# get samples from validation dataset\n",
    "val_samples = x_val.take(1)\n",
    "\n",
    "# get raw numpy arrays\n",
    "val_input = [item for pair in val_samples.as_numpy_iterator() for item in pair[0]]\n",
    "\n",
    "# and decoded\n",
    "decoded_imgs = autoencoder.predict(x=val_samples)\n",
    "\n",
    "# display\n",
    "compare_image_predictions(val_input, decoded_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
